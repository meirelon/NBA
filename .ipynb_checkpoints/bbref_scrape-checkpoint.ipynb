{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.html as LH\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "import urllib.request\n",
    "import datetime\n",
    "from itertools import compress\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2017, 10, 5, 10, 11, 15, 635636)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " class HTMLTableParser:\n",
    "        def parse_url(self, url):\n",
    "            response = requests.get(url)\n",
    "            soup = bs(response.text, 'lxml')\n",
    "            return [(table['id'],self.parse_html_table(table))\\\n",
    "                    for table in soup.find_all('table')]  \n",
    "    \n",
    "        def parse_html_table(self, table):\n",
    "            n_columns = 0\n",
    "            n_rows=0\n",
    "            column_names = []\n",
    "    \n",
    "            # Find number of rows and columns\n",
    "            # we also find the column titles if we can\n",
    "            for row in table.find_all('tr'):\n",
    "                \n",
    "                # Determine the number of rows in the table\n",
    "                td_tags = row.find_all('td')\n",
    "                if len(td_tags) > 0:\n",
    "                    n_rows+=1\n",
    "                    if n_columns == 0:\n",
    "                        # Set the number of columns for our table\n",
    "                        n_columns = len(td_tags)\n",
    "                        \n",
    "                # Handle column names if we find them\n",
    "                th_tags = row.find_all('th')\n",
    "                if len(th_tags) > 0 and len(column_names) == 0:\n",
    "                    for th in th_tags[1:]:\n",
    "                        column_names.append(th.get_text())\n",
    "    \n",
    "            # Safeguard on Column Titles\n",
    "            if len(column_names) > 0 and len(column_names) != n_columns:\n",
    "                raise Exception(\"Column titles do not match the number of columns\")\n",
    "    \n",
    "            columns = column_names if len(column_names) > 0 else range(0,n_columns)\n",
    "            df = pd.DataFrame(columns = columns,\n",
    "                              index= range(0,n_rows))\n",
    "            row_marker = 0\n",
    "            for row in table.find_all('tr'):\n",
    "                column_marker = 0\n",
    "                columns = row.find_all('td')\n",
    "                for column in columns:\n",
    "                    df.iat[row_marker,column_marker] = column.get_text()\n",
    "                    column_marker += 1\n",
    "                if len(columns) > 0:\n",
    "                    row_marker += 1\n",
    "                    \n",
    "            # Convert to float if possible\n",
    "            for col in df:\n",
    "                try:\n",
    "                    df[col] = df[col].astype(float)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            \n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class bbref_scrape:\n",
    "    def get_player_ids(url):\n",
    "        r = requests.get(url)\n",
    "        all_tags = bs(r.content, \"html.parser\")\n",
    "        tmp = [x for x in all_tags.find_all(\"td\", class_ = \"left\")]\n",
    "        ids = []\n",
    "        for x in tmp:\n",
    "            try:\n",
    "                ids.append(x[\"data-append-csv\"])\n",
    "            except:\n",
    "                next\n",
    "        return(ids)\n",
    "\n",
    "    def get_player_links(url):\n",
    "        r = requests.get(url)\n",
    "        all_tags = bs(r.content, \"html.parser\")\n",
    "        ids_bool = [bool(re.search(pattern=\"players/\\w/.+\", string = x[\"href\"])) for x in all_tags.find_all(\"a\")]\n",
    "        ids = list(compress([x[\"href\"] for x in all_tags.find_all(\"a\")], ids_bool))\n",
    "        return([re.sub(pattern=\"[.](html)\",string=x, repl=\"\") for x in ids])\n",
    "    \n",
    "    def get_player_gamelogs(sport_type, link, year):\n",
    "        def text(elt):\n",
    "            return elt.text_content().replace(u'\\xa0', u' ')\n",
    "        \n",
    "        if(sport_type == \"basketball\"):\n",
    "            ref_link = \"basketball-reference.com/\"\n",
    "            n = 30\n",
    "            tbl_xpath = '//*[@id=\"pgl_basic\"]'\n",
    "        else:\n",
    "            ref_link = \"hockey-reference.com/\"\n",
    "            n = 29\n",
    "            tbl_xpath = '//*[@id=\"gamelog\"]'\n",
    "\n",
    "        bbrefID = re.findall(string=link, pattern=\"(?<=[/])\\w+|\\d+\")[2]\n",
    "        url = \"https://www.\"+ ref_link + link + \"/gamelog/\" + str(year)\n",
    "        r = requests.get(url)\n",
    "        all_tags = LH.fromstring(r.content)\n",
    "    \n",
    "        for table in all_tags.xpath(tbl_xpath):\n",
    "            header = [text(th) for th in table.xpath('//th')][1:n]\n",
    "            data = [[text(td) for td in tr.xpath('td')]  \n",
    "                    for tr in table.xpath('//tr')][1:]\n",
    "            data = [row for row in data if len(row)==len(header)]\n",
    "            data = pd.DataFrame(data, columns = header)\n",
    "            df = pd.concat([pd.DataFrame({\"bbrefID\":[bbrefID for bbref in range(len(data))]}), data], axis=1)\n",
    "            return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hp = HTMLTableParser()\n",
    "#NBA\n",
    "player_pg = \"https://www.basketball-reference.com/leagues/NBA_2017_per_game.html\"\n",
    "player_p100 = \"https://www.basketball-reference.com/leagues/NBA_2017_per_poss.html\"\n",
    "player_p36 = \"https://www.basketball-reference.com/leagues/NBA_2017_per_minute.html\"\n",
    "player_advanced = \"https://www.basketball-reference.com/leagues/NBA_2017_advanced.html\"\n",
    "#NHL\n",
    "skater_basic = \"https://www.hockey-reference.com/leagues/NHL_2018_skaters.html\"\n",
    "skater_advanced = \"https://www.hockey-reference.com/leagues/NHL_2018_skaters-advanced.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NBA Game Logs\n",
    "year = 2017\n",
    "ids = bbref_scrape.get_player_links(player_pg)\n",
    "game_logs_df = pd.concat([bbref_scrape.get_player_gamelogs(sport_type = \"basketball\", link = x, year = year) for x in ids])\n",
    "#Create Draft Kings Score\n",
    "game_logs_df[[\"FG\", \"3P\", \"TRB\", \"AST\", \"STL\", \"BLK\", \"TOV\"]] = game_logs_df[[\"FG\", \"3P\", \"TRB\", \"AST\", \"STL\", \"BLK\", \"TOV\"]].astype(float)\n",
    "game_logs_df[\"dk\"] = (1*game_logs_df[\"FG\"]) + ((1/2)*game_logs_df[\"3P\"]) + ((5/4)*game_logs_df[\"TRB\"]) + ((3/2)*game_logs_df[\"AST\"]) + (2*game_logs_df[\"STL\"]) + (2*game_logs_df[\"BLK\"]) + ((1/2)*game_logs_df[\"TOV\"])\n",
    "double_double = pd.Series(game_logs_df[[\"FG\", \"TRB\", \"AST\", \"STL\", \"BLK\", \"TOV\"]].apply(lambda x: sum(x>=10), axis = 1) > 1)\n",
    "game_logs_df[\"dk\"][double_double] += 1.5\n",
    "#Write the csv\n",
    "game_logs_df.to_csv(\"C:\\\\Users\\\\neste\\\\Google Drive\\\\NBA-NHL\\\\game_logs\\\\nba_game_logs_%s.csv\" % str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NHL Skater Game Logs\n",
    "year = 2018\n",
    "ids = bbref_scrape.get_player_links(skater_basic)\n",
    "game_logs = [bbref_scrape.get_player_gamelogs(sport_type = \"hockey\", link = x, year = year) for x in ids]\n",
    "game_logs_new = list(compress(game_logs, [x is not None for x in game_logs]))\n",
    "game_logs_df = pd.concat(list(compress(game_logs_new, [\"Goalie Stats\" not in x.columns for x in game_logs_new])))\n",
    "game_logs_df.to_csv(\"C:\\\\Users\\\\neste\\\\Google Drive\\\\NBA-NHL\\\\game_logs\\\\nhl_game_logs_%s.csv\" % str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game_logs_df = pd.read_csv(\"C:\\\\Users\\\\neste\\\\Google Drive\\\\NBA-NHL\\\\game_logs\\\\nba_game_logs_2017.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
